{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"lib/python3.7/site-packages/\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# N_DOCUMENTS = 517402\n",
    "N_DOCUMENTS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "X = []\n",
    "name_dict = set()\n",
    "for i in range(1, N_DOCUMENTS):\n",
    "    f = open(\"emails/processed/\" + str(i + 1) + \".txt\", \"r\")\n",
    "    clean_string = \"\"\n",
    "    for line in f:\n",
    "        if line[:4] == \"From\":\n",
    "            line = line.split(\" \")\n",
    "            for element in line[1].split(\"@\")[0].split(\".\"):\n",
    "                name_dict.add(element) \n",
    "        elif line[:2] == \"To\":\n",
    "            line = line.split(\" \")\n",
    "            for element in line[1].split(\"@\")[0].split(\".\"):\n",
    "                name_dict.add(element) \n",
    "        elif line[:7] == \"Subject\":\n",
    "            line = line.split(\" \")\n",
    "            clean_string += line[1]\n",
    "        elif line[:10] == \"X-FileName\":\n",
    "            break\n",
    "    for line in f:\n",
    "        clean_string += line.strip(\"\\n\") + \"\\n\"\n",
    "    f.close()\n",
    "    X.append(clean_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'clean/1.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-8d7da3b48b73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_wordnet_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"clean/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'clean/1.txt'"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "stop_words.add(\"ect\")\n",
    "stop_words.add(\"hou\")\n",
    "stop_words.add(\"com\")\n",
    "stop_words.add(\"www\")\n",
    "stop_words.add(\"http\")\n",
    "stop_words.add(\"best\")\n",
    "stop_words.add(\"thanks\")\n",
    "stop_words.add(\"thank\")\n",
    "stop_words.add(\"dear\")\n",
    "stop_words.add(\"please\")\n",
    "for name in name_dict:\n",
    "    stop_words.add(name)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for i in range(len(X)):\n",
    "    sent_text = nltk.sent_tokenize(X[i])\n",
    "    tokenized_text = []\n",
    "    \n",
    "    # Remove the numbers\n",
    "    for sentence in sent_text:\n",
    "        sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence)\n",
    "        tokenized_text += list(nltk.word_tokenize(sentence))\n",
    "        \n",
    "    # Remove stopwords\n",
    "    X[i] = [x.lower() for x in tokenized_text if x.lower() not in stop_words and len(x) > 2]\n",
    "    \n",
    "    # Pos Tag    \n",
    "    X[i] = [x for x in nltk.pos_tag(X[i])]\n",
    "    \n",
    "    # Lemmatize\n",
    "    X[i] = list(map(lambda x: lemmatizer.lemmatize(x[0], pos = get_wordnet_pos(x[1])), X[i]))\n",
    "    \n",
    "    f = open(\"clean/\"+str(i+1)+\".txt\", \"w\")\n",
    "    f.write(\" \".join(X[i]))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_emails = X\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_emails)\n",
    "dictionary.filter_extremes(no_above=0.5)\n",
    "bag_of_words = [dictionary.doc2bow(email) for email in processed_emails]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf_object = models.TfidfModel(bag_of_words)\n",
    "tfidf_vectors = tfidf_object[bag_of_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.models import LsiModel\n",
    "j_index = 0\n",
    "for top in range(10, 15):\n",
    "    lda_model = LdaMulticore(tfidf_vectors, num_topics=top, id2word=dictionary)\n",
    "    lsa_model = LsiModel(tfidf_vectors,num_topics=top, id2word=dictionary) \n",
    "    lda_coherence = CoherenceModel(model=lda_model, texts = processed_emails, dictionary=dictionary, coherence='c_v')\n",
    "    lsa_coherence = CoherenceModel(model=lsa_model, texts = processed_emails, dictionary=dictionary, coherence='c_v')\n",
    "\n",
    "    lD_name = \"modeltest/LDA/lda\" + str(j_index) + \".model\"\n",
    "    lD_coh = \"modeltest/LDA/lda\" + str(j_index) + \".coherence\"\n",
    "    \n",
    "    lS_name = \"modeltest/LSA/lsa\" + str(j_index) + \".model\"\n",
    "    lS_coh = \"modeltest/LSA/lsa\" + str(j_index) + \".coherence\"\n",
    "\n",
    "    # save the models to the disk\n",
    "    lda_model.save(lD_name)\n",
    "    lda_coherence.save(lD_coh)\n",
    "\n",
    "    lsa_model.save(lS_name)\n",
    "    lsa_coherence.save(lS_coh)\n",
    "\n",
    "    j_index = j_index+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "for t in range(0, lda_model.num_topics-1):\n",
    "    print(t)\n",
    "#     print(lda_model.show_topic(t, 80))\n",
    "#     print(' - '.join([v[0] for v in load_model.show_topic(t, 20)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
